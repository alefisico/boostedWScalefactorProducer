; Simple example how to run over datasets

[global]
variable markers   = @
task        = UserTask      ; Job uses user written scripts
backend     = slurm         ; Send to local batch system
workdir = $CMSSW_BASE/src/WTopScalefactorProducer/Skimmer/newsubmission/gctest

[local]
queue = wn
scratch path = /scratch

[jobs]
wall time   = 8:00:00          ; Jobs will take max 8h
in flight = 600

[UserTask]
executable  = runProduction.sh   ; Name of the script
dataset     = 
	catalogue/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8.txt
dataset splitter = FileBoundarySplitter
dataset refresh  = 4:00
input files = env.sh ;common.sh
files per job = 10                   ; Number of files to process per job
[storage]
se output files = out.root
se output pattern = job_@MY_JOBID@_@X@
se path = srm://t3se01.psi.ch:8443/srm/managerv2?SFN=/pnfs/psi.ch/cms/trivcat/store/user/$USER/production/Wtagging/${GC_TASK_ID}/${DATASETPATH}/
;nickname source = Example05_dataset.MyNick
;dataset check nickname collision = ignore
;dataset refresh = 1
;partition processor += MetaPartitionProcessor
;partition metadata = META_KEY1 META_KEY2 META_KEY3

