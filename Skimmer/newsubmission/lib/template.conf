; Simple example how to run over datasets

[global]
variable markers   = @
task        = UserTask      ; Job uses user written scripts
backend     = slurm         ; Send to local batch system
workdir = $workdir$

[local]
queue = $queue$
scratch path = $scratchpath$

[jobs]
wall time   = $maxtime$:00:00          ; Jobs will take max 8h
in flight = 600

[UserTask]
executable  = runProduction.sh   ; Name of the script
dataset     = 
$dataset$
dataset splitter = FileBoundarySplitter
dataset refresh  = 4:00
input files = env.sh ;common.sh
files per job = $multiplicity$                   ; Number of files to process per job
[storage]
se output files = out.root
se output pattern = job_@MY_JOBID@_@X@
se path = $sepath$/${GC_TASK_ID}/${DATASETPATH}/
;nickname source = Example05_dataset.MyNick
;dataset check nickname collision = ignore
;dataset refresh = 1
;partition processor += MetaPartitionProcessor
;partition metadata = META_KEY1 META_KEY2 META_KEY3

