; Template of Grid Control (GC) configuration script used for production 

[global]
variable markers   = @
task        = UserTask      ; Job uses user written scripts
backend     = slurm         ; Send to local batch system (slurm)
workdir = $workdir$			; The work directory where the logs and information about the jobs for GC are stored 

[condor]
use_x509userproxy = true

[local]
queue = wn					; The name of the queue to which submit the jobs
scratch path = /scratch	; The path to the scratch directory on the worker node 

[backend]
proxy = VomsProxy

[jobs]
wall time   = $maxtime$:00:00 	; Max amount of time each job will take
in flight = 500 				; Maximum number of jobs submitted at the same time 
max retry = 10 			; Maximum number of resubmissions for failed jobs 

[constants]
LOCAL_OUTPUT_DIR = /work/mhuwiler/data/WScaleFactors/production

[UserTask]
executable  = executable.sh   	; Name of the script
dataset     = 					; List of GC dataset files 
$dataset$
dataset splitter = FileBoundarySplitter
dataset refresh  = 4:00
input files = env.sh 			 	; Additional files to be transferred (env.sh is generated in makeJobConfig.py)
files per job = $multiplicity$  	; Number of files to process per job

[Task]
Memory = 2000

[storage]
se output files = out.root qualitycheck.pkl timing.dat			; The name of the files to transfer back from the scratch area 
se output pattern = job_@MY_JOBID@_@X@
se path = srm://t3se01.psi.ch:8443/srm/managerv2?SFN=/pnfs/psi.ch/cms/trivcat/store/user/$USER/production/Wtagging/${GC_TASK_ID}/${DATASETPATH}/

